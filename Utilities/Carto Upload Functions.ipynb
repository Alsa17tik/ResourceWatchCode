{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions needed to bulk insert rows into Carto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_api(url, sql, key):\n",
    "    \"\"\" Execute sql request over Carto SQL API \"\"\"\n",
    "    params = {\n",
    "        'api_key' : key,\n",
    "        'q'       : sql\n",
    "    }\n",
    "    r = req.get(url, params=params)\n",
    "    return(r)\n",
    "\n",
    "def dump_row_contents(row, cols_and_types):\n",
    "    \"\"\" Format data from a dataframe for insert statements into a Carto table \"\"\"\n",
    "    dump = \"(\"\n",
    "    for ix in row.index:\n",
    "        if cols_and_types[ix] in [\"date\", \"varchar\"]:\n",
    "            dump += \"'\" + str(row[ix]) + \"',\"\n",
    "        else:\n",
    "            dump += str(row[ix]) + \",\"\n",
    "    dump = dump[:-1]+\")\"\n",
    "    return(dump)\n",
    "\n",
    "def update_in_batches(data_df, batch_size, target_table_name, cols_and_types, cols_with_apostrophes=None):\n",
    "    \"\"\" \n",
    "    Send new rows for Carto in smaller batch sizes.\n",
    "    A batch_size of 20 seems to work for the location data. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define column names\n",
    "    columns = str(tuple(data_df.columns)).replace(\"'\",\"\")\n",
    "    \n",
    "    # Determine number of batches in which to send data\n",
    "    num_batches = int(data_df.shape[0] / batch_size)\n",
    "\n",
    "    for batch in range(num_batches+1):\n",
    "        # Select sub-dataframe\n",
    "        sub_df = data_df.iloc[batch*batch_size:batch*batch_size+batch_size]\n",
    "        \n",
    "        # Replace apostrophes from varchar columns with &#8217\n",
    "        sub_df = toggle_apostrophes(sub_df, cols_with_apostrophes, remove=True)\n",
    "        \n",
    "        # Create Insert SQL statement\n",
    "        values = \", \".join(list(sub_df.apply(lambda row: dump_row_contents(row, cols_and_types), axis=1)))\n",
    "        insert_value_sql = \"\"\"\n",
    "        INSERT INTO {table_name} {columns} VALUES {values}\n",
    "        \"\"\".format(table_name=table_name, columns=columns, values=values)\n",
    "\n",
    "        res = sql_api(carto_url, insert_value_sql, carto_api_token)\n",
    "\n",
    "        # Help with trouble shooting\n",
    "        # Display response error, the data that created the error, and break the cycle\n",
    "        if \"error\" in res.text:\n",
    "            print(res.text)\n",
    "            print(sub_df)\n",
    "            break\n",
    "\n",
    "        print(\"Completed up until index:\", batch*batch_size+batch_size)\n",
    "        \n",
    "def keep_geolocated(df):\n",
    "    \"\"\"Throw away points that do not have a latitude and longitude defined\"\"\"\n",
    "    keep_geotagged = pd.notnull(df[\"latitude\"]) & pd.notnull(df[\"longitude\"]) \n",
    "    df = df.loc[keep_geotagged]\n",
    "    return(df)\n",
    "\n",
    "def fix_precision_of_floats(df, float_columns, precision):\n",
    "    \"\"\"Use this to address problem of comparing numpy floats with rounding errors\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in float_columns:\n",
    "        df[col] = np.around(df[col],precision)\n",
    "    return(df)\n",
    "\n",
    "def toggle_apostrophes(df, cols_with_apostrophes, remove=True):\n",
    "    \"\"\"\n",
    "    Will switch between &#8217 and ' representation of an apostrophe\n",
    "    Provides a reversible function to accomplish this\n",
    "    \n",
    "    TO DO: Address how this affects our data storage... &#8217 will be in Carto table\n",
    "    \n",
    "    \"\"\"\n",
    "    # Copy df\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize array to avoid \"NoneType not iterable\" error\n",
    "    if not cols_with_apostrophes:\n",
    "        cols_with_apostrophes = []\n",
    "    \n",
    "    # Loop over all columns and either remove or replace apostrophes\n",
    "    for col in cols_with_apostrophes:\n",
    "        if remove:\n",
    "            df[col] = df[col].apply(lambda row: str(row).replace(\"'\", \"&#8217\"))\n",
    "        else:\n",
    "            df[col] = df[col].apply(lambda row: str(row).replace(\"&#8217\", \"'\"))\n",
    "    return(df)\n",
    "\n",
    "def update_table_without_duplicates(data_df, target_table_name, cols_and_types, float_cols=[\"latitude\", \"longitude\"], precision=8, cols_with_apostrophes=None, consider_partial_history=None):\n",
    "    \"\"\" \n",
    "    Determines whether there are new locations to add to the table.\n",
    "    Sends an SQL statement and returns the result of that operation to stdout.\n",
    "    \n",
    "    look_back_length parameter allows for de-duping with a table while only considering a limited\n",
    "    history of the recent record\n",
    "    \"\"\"\n",
    "    # column names to include\n",
    "    column_names = list(cols_and_types.keys())\n",
    "    \n",
    "    # Read in existing table, set look_back if desired\n",
    "    # Particularly for updating OpenAQ history, as opposed to locations list (look_back_length=None)\n",
    "    # NOTE: this is confusing parameter use. Consider a better formulation.\n",
    "    \n",
    "    if not consider_partial_history:\n",
    "        # Update against entire table\n",
    "        select_all_sql = \"\"\"\n",
    "        SELECT * FROM {table_name}\n",
    "        \"\"\".format(table_name=target_table_name)\n",
    "        res = sql_api(carto_url, select_all_sql, carto_api_token)\n",
    "    else:\n",
    "        \n",
    "        ##\n",
    "        ### TO DO: ensure that this works!\n",
    "        ##\n",
    "        \n",
    "        # Update against partial history\n",
    "        select_all_sql = \"\"\"\n",
    "        SELECT * FROM {table_name} WHERE lastUpdated < {length_of_partial_history}\n",
    "        \"\"\".format(table_name=target_table_name, length_of_partial_history=consider_partial_history)\n",
    "        res = sql_api(carto_url, select_all_sql, carto_api_token)\n",
    "        \n",
    "    target_table = pd.DataFrame(res.json()[\"rows\"], columns=column_names)\n",
    "    \n",
    "    # Fix precision on float columns\n",
    "    target_table = fix_precision_of_floats(target_table, float_cols, 8)\n",
    "    \n",
    "    # Fix apostrophe change\n",
    "    target_table = toggle_apostrophes(target_table, cols_with_apostrophes, remove=False)\n",
    "    \n",
    "    # Determine unique observations in data_df (de-dupe in the new observations)\n",
    "    # http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.drop_duplicates.html\n",
    "    obs = data_df[column_names] #.set_index(index_cols)\n",
    "    obs = obs.drop_duplicates(keep=\"first\")\n",
    "    obs = keep_geolocated(obs)\n",
    "    obs = fix_precision_of_floats(obs, float_cols, 8)\n",
    "    \n",
    "    # De-dupe between existing table and new observations\n",
    "    # https://stackoverflow.com/questions/29464234/compare-python-pandas-dataframes-for-matching-rows\n",
    "    shared = pd.merge(target_table, obs, on=column_names, how=\"inner\")\n",
    "    shared[\"key\"] = \"x\"\n",
    "    temp_df = pd.merge(obs, shared, on=column_names, how=\"left\")\n",
    "    new_obs = temp_df[temp_df[\"key\"].isnull()].drop(\"key\", axis=1)\n",
    "\n",
    "    print(\"Number of new observations added to\", target_table_name + \":\", new_obs.shape[0])\n",
    "    \n",
    "    # Add genuinely new observations to the existing table\n",
    "    update_in_batches(new_obs, 20, target_table_name, cols_and_types, cols_with_apostrophes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
