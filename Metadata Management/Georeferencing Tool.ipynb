{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cartoframes\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "import requests as req\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "import sys\n",
    "#import logging\n",
    "import os\n",
    "#logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ADDITIONAL_ALIASES = '/Users/nathansuberi/Documents/GitHub/ResourceWatchCode/Metadata Management/aliases_for_longform.csv'\n",
    "CARTO_USER = 'wri-rw'#os.environ.get('CARTO_USER')\n",
    "CARTO_KEY = ''#os.environ.get('CARTO_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure connection to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = #os.environ.get('aws_access_key_id')\n",
    "aws_secret_access_key = #os.environ.get('aws_secret_access_key')\n",
    "\n",
    "s3_bucket = \"wri-public-data\"\n",
    "s3_folder = \"resourcewatch/wide_to_long/\"\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "# Functions for reading and uploading data to/from S3\n",
    "def read_from_S3(bucket, key, index_col=0):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(io.BytesIO(obj['Body'].read()), index_col=[index_col], encoding=\"utf8\")\n",
    "    return(df)\n",
    "\n",
    "def write_to_S3(df, bucket, key):\n",
    "    csv_buffer = io.StringIO()\n",
    "    # Need to set encoding in Python2... default of 'ascii' fails\n",
    "    df.to_csv(csv_buffer, encoding='utf-8')\n",
    "    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Carto connection and load georef tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc = cartoframes.CartoContext(base_url='https://{}.carto.com/'.format(CARTO_USER),\n",
    "                              api_key=CARTO_KEY)\n",
    "georef = {\n",
    "    'geometry':cc.read('wri_countries_a'),\n",
    "    'aliases':cc.read('country_aliases_extended')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wri_id</th>\n",
       "      <th>rw_id</th>\n",
       "      <th>country_name</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com.009</td>\n",
       "      <td>c61c364b-1d68-4dd9-ae3d-76c2a0022280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>isoalpha3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cit.013</td>\n",
       "      <td>5d269c36-6ccf-4620-838d-431f86c30f69</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cit.020</td>\n",
       "      <td>6d3163f5-4e08-4830-84f1-2c5d76570a82</td>\n",
       "      <td>country_name</td>\n",
       "      <td>country_code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cli.022</td>\n",
       "      <td>995ec4fe-b3cc-4cf4-bd48-b89d4e3ea072</td>\n",
       "      <td>countryname</td>\n",
       "      <td>iso3v10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ene.012</td>\n",
       "      <td>d446a52e-c4c1-4e74-ae30-3204620a0365</td>\n",
       "      <td>country_name</td>\n",
       "      <td>country_code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for.020</td>\n",
       "      <td>03bfb30e-829f-4299-bab9-b2be1b66b5d4</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>soc.001</td>\n",
       "      <td>0b9f0100-ce5b-430f-ad8f-3363efa05481</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>soc.002</td>\n",
       "      <td>d4ca3cc4-c162-469c-b341-b52284a73eaa</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>soc.012</td>\n",
       "      <td>f48541d3-a622-4908-9400-5ef26257ac96</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>soc.021</td>\n",
       "      <td>e7582657-9c16-4eb1-89e8-0211d94015c6</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>soc.022</td>\n",
       "      <td>773a16a7-3531-4b56-8253-babd15ad7f87</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>soc.024</td>\n",
       "      <td>6c6e70e7-5a19-46f2-9d95-34789fd20adc</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>soc.026</td>\n",
       "      <td>0be2ce12-79b3-434b-b557-d6ea92d787fe</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>soc.045</td>\n",
       "      <td>2cc29514-b97a-4103-92b1-c8c8e9268cd8</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>soc.055</td>\n",
       "      <td>795a7ceb-ebc1-4479-95ad-76ea4d045ad3</td>\n",
       "      <td>country</td>\n",
       "      <td>iso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soc.067</td>\n",
       "      <td>1de2af1c-5e5e-4a33-b8f1-8c8f9d000e49</td>\n",
       "      <td>country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>com.022</td>\n",
       "      <td>c2142922-84d9-4564-8216-a4867b9e48c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wri_id                                  rw_id  country_name  country_code\n",
       "0   com.009   c61c364b-1d68-4dd9-ae3d-76c2a0022280           NaN     isoalpha3\n",
       "1   cit.013   5d269c36-6ccf-4620-838d-431f86c30f69       country           NaN\n",
       "2   cit.020   6d3163f5-4e08-4830-84f1-2c5d76570a82  country_name  country_code\n",
       "3   cli.022   995ec4fe-b3cc-4cf4-bd48-b89d4e3ea072   countryname       iso3v10\n",
       "4   ene.012   d446a52e-c4c1-4e74-ae30-3204620a0365  country_name  country_code\n",
       "5   for.020   03bfb30e-829f-4299-bab9-b2be1b66b5d4       country           NaN\n",
       "6   soc.001   0b9f0100-ce5b-430f-ad8f-3363efa05481       country           NaN\n",
       "7   soc.002   d4ca3cc4-c162-469c-b341-b52284a73eaa       country           NaN\n",
       "8   soc.012   f48541d3-a622-4908-9400-5ef26257ac96       country           NaN\n",
       "9   soc.021   e7582657-9c16-4eb1-89e8-0211d94015c6       country           NaN\n",
       "10  soc.022   773a16a7-3531-4b56-8253-babd15ad7f87       country           NaN\n",
       "11  soc.024   6c6e70e7-5a19-46f2-9d95-34789fd20adc       country           NaN\n",
       "12  soc.026   0be2ce12-79b3-434b-b557-d6ea92d787fe       country           NaN\n",
       "13  soc.045   2cc29514-b97a-4103-92b1-c8c8e9268cd8       country           NaN\n",
       "14  soc.055   795a7ceb-ebc1-4479-95ad-76ea4d045ad3       country           iso\n",
       "15  soc.067   1de2af1c-5e5e-4a33-b8f1-8c8f9d000e49       country           NaN\n",
       "16  com.022   c2142922-84d9-4564-8216-a4867b9e48c5           NaN           iso"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data sets info from config file\n",
    "georef_config = pd.read_csv('/Users/nathansuberi/Desktop/RW_Data/georeferencing_tasks/georef_these.csv')\n",
    "georef_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data sets into memory for processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from RW API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Carto datasets: 237\n"
     ]
    }
   ],
   "source": [
    "# Base URL for getting dataset metadata from RW API\n",
    "url = \"https://api.resourcewatch.org/v1/dataset?sort=slug,-provider,userId&status=saved&includes=metadata,vocabulary,widget,layer\"\n",
    "\n",
    "# page[size] tells the API the maximum number of results to send back\n",
    "# There are currently between 200 and 300 datasets on the RW API\n",
    "payload = { \"application\":\"rw\", \"page[size]\": 1000}\n",
    "\n",
    "# Request all datasets, and extract the data from the response\n",
    "res = req.get(url, params=payload)\n",
    "data = res.json()[\"data\"]\n",
    "\n",
    "### Convert the json object returned by the API into a pandas DataFrame\n",
    "# Another option: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "datasets_on_api = {}\n",
    "for ix, dset in enumerate(data):\n",
    "    atts = dset[\"attributes\"]\n",
    "    metadata = atts[\"metadata\"]\n",
    "    layers = atts[\"layer\"]\n",
    "    widgets = atts[\"widget\"]\n",
    "    tags = atts[\"vocabulary\"]\n",
    "    datasets_on_api[dset[\"id\"]] = {\n",
    "        \"name\":atts[\"name\"],\n",
    "        \"table_name\":atts[\"tableName\"],\n",
    "        \"provider\":atts[\"provider\"],\n",
    "        \"date_updated\":atts[\"updatedAt\"],\n",
    "        \"num_metadata\":len(metadata),\n",
    "        \"metadata\": metadata,\n",
    "        \"num_layers\":len(layers),\n",
    "        \"layers\": layers,\n",
    "        \"num_widgets\":len(widgets),\n",
    "        \"widgets\": widgets,\n",
    "        \"num_tags\":len(tags),\n",
    "        \"tags\":tags\n",
    "    }\n",
    "\n",
    "# Create the DataFrame, name the index, and sort by date_updated\n",
    "# More recently updated datasets at the top\n",
    "current_datasets_on_api = pd.DataFrame.from_dict(datasets_on_api, orient='index')\n",
    "current_datasets_on_api.index.rename(\"Dataset\", inplace=True)\n",
    "current_datasets_on_api.sort_values(by=[\"date_updated\"], inplace=True, ascending = False)\n",
    "\n",
    "# Select all Carto datasets on the API:\n",
    "provider = \"cartodb\"\n",
    "carto_ids = (current_datasets_on_api[\"provider\"]==provider)\n",
    "carto_data = current_datasets_on_api.loc[carto_ids]\n",
    "\n",
    "print(\"Number of Carto datasets: \" + str(carto_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the alias table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['alias', 'index', 'iso', 'name', 'the_geom'], dtype='object')\n",
      "Index(['alias', 'iso', 'name'], dtype='object')\n",
      "Index(['alias', 'iso', 'name'], dtype='object')\n",
      "(305, 3)\n",
      "Index(['iso', 'name', 'alias'], dtype='object')\n",
      "(193, 3)\n",
      "Size of current aliasing table: (305, 3)\n"
     ]
    }
   ],
   "source": [
    "df = georef['aliases']\n",
    "print(df.columns)\n",
    "try:\n",
    "    df = df.drop('the_geom', axis=1)\n",
    "except:\n",
    "    logging.info('could not delete column `the_geom`')\n",
    "\n",
    "\n",
    "try:\n",
    "    df = df.drop('index', axis=1)\n",
    "except:\n",
    "    print('could not delete column `index`')\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "## Adding all countries from our wri-bounds shapefile to the alias table\n",
    "\n",
    "new_aliases = georef['geometry'][['iso_a3', 'name']].copy()\n",
    "new_aliases['alias'] = new_aliases['name']\n",
    "cols = ['iso' if col=='iso_a3' else col for col in new_aliases.columns]\n",
    "cols = [col.strip() for col in cols]\n",
    "new_aliases.columns = cols\n",
    "\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "print(new_aliases.columns)\n",
    "print(new_aliases.shape)\n",
    "\n",
    "df = df.append(new_aliases)\n",
    "\n",
    "## Adding in new aliases identified by Peter\n",
    "\n",
    "peters_new_aliases = pd.read_csv(ADDITIONAL_ALIASES, header=1)\n",
    "peters_new_aliases.columns = ['alias', 'name', 'iso']\n",
    "\n",
    "df = df.append(peters_new_aliases)\n",
    "\n",
    "# Make all aliases lower case, remove spacing\n",
    "df['alias'] = [alias.strip().lower().replace(' ','') for alias in df['alias']]\n",
    "\n",
    "## check / remove duplicates\n",
    "sum(df.duplicated(subset=['alias']))\n",
    "sum(df.duplicated(subset=['name']))\n",
    "sum(df.duplicated(subset=['iso']))\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "    \n",
    "georef['aliases'] = df\n",
    "\n",
    "print('Size of current aliasing table: ' + str(georef['aliases'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform georeferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tracking all mis-matched names\n",
    "missed_names = []\n",
    "missed_isos = []\n",
    "\n",
    "alias_info = georef['aliases']\n",
    "\n",
    "for name, info in tables.items():\n",
    "    \n",
    "    print('Processing table ' + name)\n",
    "    print('Table head:')\n",
    "    \n",
    "    ### WARNING: non standardized indices in the data cause problems after the merge step\n",
    "    if 'long_data' in info:\n",
    "        data = info['long_data'].copy()\n",
    "        data.index = list(range(data.shape[0]))\n",
    "    else:\n",
    "        data = info['data'].copy()\n",
    "        data.index = list(range(data.shape[0]))\n",
    "    \n",
    "    print(data.head(5))\n",
    "\n",
    "    c_code = table_info[name]['config_options'].get('country_code', False)\n",
    "    c_name = table_info[name]['config_options'].get('country_name', False)\n",
    "    \n",
    "    print('c_code: ***' + str(c_code) + '***')\n",
    "    print('c_name: ***' + str(c_name) + '***')\n",
    "    \n",
    "    \n",
    "    # Check if isos match our table\n",
    "    \n",
    "    if c_code:\n",
    "        print('already has an iso3 code, in column ' + c_code)\n",
    "        \n",
    "        df = data.merge(alias_info,\n",
    "                           left_on=c_code,\n",
    "                           right_on='iso', \n",
    "                           how='left')\n",
    "        \n",
    "        null_isos = pd.isnull(df['iso'])\n",
    "        \n",
    "        no_iso_match = df[null_isos.values]\n",
    "        print('no match for these isos in the data being processed: ')\n",
    "        print(no_iso_match[c_code].unique())\n",
    "        missed_isos.extend(no_iso_match[c_code].unique())\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # If country name is supplied, check how many match up with alias/name in country_aliases\n",
    "    if c_name:       \n",
    "        # Ensure that leading or trailing spaces don't break the match\n",
    "        data[c_name] = ['North Korea' if name=='Korea, Dem. People\\x92s Rep.' else name for name in data[c_name]]\n",
    "    \n",
    "        _data = data.copy()\n",
    "        _data['join_col'] = data[c_name].apply(lambda item: item.strip().lower().replace(' ',''))\n",
    "    \n",
    "        data_with_alias = _data.merge(alias_info, \n",
    "                                         left_on = 'join_col',\n",
    "                                         right_on = 'alias',\n",
    "                                         how='left') \n",
    "\n",
    "        null_aliases = pd.isnull(data_with_alias['alias'])             \n",
    "            \n",
    "        print('data with alias df:')\n",
    "        print(data_with_alias.shape)\n",
    "        print(data_with_alias.head(6))\n",
    "        print('raw data')\n",
    "        print(data.shape)\n",
    "        print(data.head(5))\n",
    "    \n",
    "        \n",
    "        if sum(null_aliases):\n",
    "            no_alias_match = data_with_alias.loc[null_aliases]\n",
    "            print('missed aliases, matching on column \"alias\" of country_aliases')\n",
    "            print(no_alias_match)\n",
    "            try:\n",
    "                print(no_alias_match[c_name].unique())\n",
    "                missed_names.extend(no_alias_match[c_name].unique())\n",
    "            except:\n",
    "                c_name = c_name+'_x'\n",
    "                print(no_alias_match[c_name].unique())\n",
    "                missed_names.extend(no_alias_match[c_name].unique())\n",
    "                \n",
    "        ### data IS ALTERED HERE\n",
    "\n",
    "        try:\n",
    "            data['rw_country_code'] = data_with_alias['iso']\n",
    "        except:\n",
    "            data['rw_country_code'] = data_with_alias['iso_y']\n",
    "            \n",
    "        try:\n",
    "            data['rw_country_name'] = data_with_alias['name']  \n",
    "        except:\n",
    "            data['rw_country_name'] = data_with_alias['name_y'] \n",
    "\n",
    "        \n",
    "    ### SUCCESS\n",
    "    print('Final head:')\n",
    "    print(data.head(5))\n",
    "    tables[name]['geo_data'] = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for missed names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missed_names = list(set(missed_names))\n",
    "print(len(missed_names))\n",
    "print(missed_names)\n",
    "\n",
    "\n",
    "missed_isos = list(set(missed_isos))\n",
    "print(len(missed_isos))\n",
    "print(missed_isos)\n",
    "\n",
    "for name in missed_names:\n",
    "    for alias in tables['country_aliases']['data']['alias']:\n",
    "        if alias.lower().replace(' ', '') in name.lower().replace(' ', ''):\n",
    "            print(name)\n",
    "df = tables['country_aliases']['data']  \n",
    "df.loc[df['name'].str.lower().str.contains('cur')]\n",
    "\n",
    "for iso in missed_isos:\n",
    "    for iso in tables['geometry']['data']['adm0_a3']:\n",
    "        if alias.lower().replace(' ', '') in name.lower().replace(' ', ''):\n",
    "            print(name)\n",
    "            \n",
    "for iso in missed_isos:\n",
    "    for iso in tables['country_aliases']['data']['iso']:\n",
    "        if alias.lower().replace(' ', '') in name.lower().replace(' ', ''):\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading finished files to Carto and S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, info in tables.items():\n",
    "    \n",
    "    print(name)\n",
    "    for key, data in info.items():\n",
    "        if key == 'geo_data':\n",
    "            print(data.head())\n",
    "            write_to_S3(data,s3_bucket,s3_folder+name+'_long')\n",
    "            print('saved ' + name + ' long data to s3')\n",
    "            cc.write(data, name + '_georefed_&_longform', overwrite=True)\n",
    "            print('saved ' + name + ' long data to Carto')\n",
    "            #print('failed to write table ' + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Layers on Backoffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
