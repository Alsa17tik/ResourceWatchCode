{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Download Google Spreadsheets ####\n",
    "\n",
    "# Make a temporary directory, and move into it\n",
    "!mkdir temp\n",
    "os.chdir(\"temp\")\n",
    "dest = os.getcwd()\n",
    "\n",
    "# July Data Sheet\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1viPOGYIk6RGu7YMoM3BHNVbkWaCZ0JFBOMSNncWvHYk/export?format=tsv\" > july_data_upload.tsv\n",
    "july_data_upload = pd.read_csv(dest+\"/july_data_upload.tsv\", sep=\"\\t\", index_col=[0])\n",
    "\n",
    "# # Metadata to Upload\n",
    "# !curl \"https://docs.google.com/spreadsheets/d/1UkABgMlBIinJjITa6WepFAL-8VBkulS0LCbKojRXjVY/export?format=tsv\" > current_metadata.tsv\n",
    "# current_mdata = pd.read_csv(dest+\"/current_metadata.tsv\", sep=\"\\t\", index_col=[0])\n",
    "# current_mdata = current_mdata.transpose()\n",
    "\n",
    "!curl \"https://docs.google.com/spreadsheets/d/1UkABgMlBIinJjITa6WepFAL-8VBkulS0LCbKojRXjVY/export?format=tsv\" > current_metadata.tsv\n",
    "current_mdata = pd.read_csv(dest+\"/current_metadata.tsv\", sep=\"\\t\", index_col=[0])\n",
    "current_mdata = current_mdata.transpose()\n",
    "\n",
    "# Delete temporary files\n",
    "os.chdir(\"..\")\n",
    "!rm -r temp\n",
    "\n",
    "#### Merge info from the Tracking sheet and Metadata sheet\n",
    "\n",
    "# Add RW API key to metadata_to_upload\n",
    "def fetch_info(row, match_col, dest_df, target_column):\n",
    "    try:\n",
    "        info = dest_df.loc[row[match_col], target_column]\n",
    "        return(info)\n",
    "    except:\n",
    "        return(None)\n",
    "\n",
    "old_id_col = \"VIZZ - RW API (bulk upload)\"\n",
    "new_id_col = \"API-ID (PERFECT DATASET)\"\n",
    "dl_from_src_col = \"Download from Source\"\n",
    "dl_from_s3_col = \"Download Data (S3)\"\n",
    "\n",
    "match_col = \"Unique ID\"\n",
    "\n",
    "current_mdata[old_id_col] = current_mdata.apply(lambda row: fetch_info(row,match_col,july_data_upload,old_id_col), axis=1)\n",
    "current_mdata[new_id_col] = current_mdata.apply(lambda row: fetch_info(row,match_col,july_data_upload,new_id_col), axis=1)\n",
    "\n",
    "current_mdata[dl_from_src_col] = current_mdata.apply(lambda row: fetch_info(row,match_col,july_data_upload,dl_from_src_col), axis=1)\n",
    "current_mdata[dl_from_s3_col] = current_mdata.apply(lambda row: fetch_info(row,match_col,july_data_upload,dl_from_s3_col), axis=1)\n",
    "\n",
    "# Keep only those datasets with rw_ids already\n",
    "valid_old_ids = pd.notnull(current_mdata[old_id_col])\n",
    "valid_new_ids = pd.notnull(current_mdata[new_id_col])\n",
    "\n",
    "def choose_new_id(df, valid_old_ids,old_id_col, valid_new_ids,new_id_col):\n",
    "    assert(len(valid_old_ids)==len(valid_new_ids))\n",
    "    final_ids = []\n",
    "    for i in range(0, len(valid_new_ids)):\n",
    "        if(valid_new_ids[i]):\n",
    "            final_ids.append(df.iloc[i][new_id_col])\n",
    "        elif(valid_old_ids[i]):\n",
    "            final_ids.append(df.iloc[i][old_id_col])\n",
    "        else:\n",
    "            final_ids.append(None)\n",
    "    return(final_ids)\n",
    "\n",
    "current_mdata[\"final_ids\"] = choose_new_id(current_mdata, valid_old_ids,old_id_col, valid_new_ids,new_id_col)\n",
    "keep_matched_ids = pd.notnull(current_mdata[\"final_ids\"])\n",
    "\n",
    "current_mdata = current_mdata.loc[keep_matched_ids]\n",
    "current_mdata.set_index(\"final_ids\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1 = [None, \"a\", None, \"b\"]\n",
    "list2 = [\"c\", None, None, \"d\"]\n",
    "df = pd.DataFrame({\"l1\":list1, \"l2\":list2})\n",
    "choose_new_id(df, list1,\"l1\", list2,\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(current_mdata.shape)\n",
    "print(july_data_upload.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(valid_new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_mdata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "july_data_upload.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_mdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FOR EACH DATASET IN BACKOFFICE THAT HAS METADATA, UPLOAD IT\n",
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.read(\"../.env\")\n",
    "api_token = config.get(\"auth\", \"api_token\")\n",
    "\n",
    "auth_token = api_token # <Insert Auth Token Here>\n",
    "def clean_nulls(val):\n",
    "    try:\n",
    "        if np.isnan(val):\n",
    "            return(None)\n",
    "        else:\n",
    "            return(val)\n",
    "    except:\n",
    "        return(val)\n",
    "\n",
    "#test = [\"3624554e-b240-4edb-9110-1f010642c3f3\"]\n",
    "\n",
    "\n",
    "\n",
    "processed1 = []\n",
    "\n",
    "\n",
    "### THIS ADDS ALL DATASETS FOR WHICH WE HAVE METADATA in METADATA FOR UPLOAD ###\n",
    "\n",
    "small_batch = [\"5e69cfac-1f68-4864-a19a-3c1bdb180100\"]\n",
    "#for rw_id in current_mdata.index:\n",
    "for rw_id in small_batch:\n",
    "    url = \"https://api.resourcewatch.org/v1/dataset/\"+str(rw_id)+\"/metadata\"\n",
    "    print(url)\n",
    "    # Everything from current_mdata\n",
    "    metadata = current_mdata.loc[rw_id]\n",
    "    if len(metadata.shape) > 1:\n",
    "        print(metadata)\n",
    "    \n",
    "    #print(metadata)\n",
    "    row_payload = {\n",
    "        \"language\": \"en\",\n",
    "        \n",
    "        \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "        \"description\": clean_nulls(metadata[\"Description\"]),\n",
    "        \"source\": clean_nulls(metadata[\"Subtitle\"]),\n",
    "        \"functions\": clean_nulls(metadata[\"Function\"]),\n",
    "        \n",
    "        \"application\":\"rw\",\n",
    "        \"dataset\":rw_id,\n",
    "        \n",
    "        \"info\": {\n",
    "            \n",
    "            \"wri_rw_id\": clean_nulls(metadata[\"Unique ID\"]),\n",
    "            \"data_type\": clean_nulls(metadata[\"Data Type\"]),\n",
    "\n",
    "            \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "            \"source_organization\": clean_nulls(metadata[\"Source Organizations\"]),\n",
    "            \"technical_title\":clean_nulls(metadata[\"Technical Title\"]),\n",
    "\n",
    "            \"function\": clean_nulls(metadata[\"Function\"]),\n",
    "            \"cautions\": clean_nulls(metadata[\"Cautions\"]),\n",
    "            \n",
    "            \"citation\": clean_nulls(metadata[\"Citation\"]),\n",
    "            \"summary_of_license\": clean_nulls(metadata[\"Summary of Licence\"]),\n",
    "            \"link_to_license\": clean_nulls(metadata[\"Link to License\"]),\n",
    "            \n",
    "            \"geographic_coverage\": clean_nulls(metadata[\"Geographic Coverage\"]),\n",
    "            \"spatial_resolution\": clean_nulls(metadata[\"Spatial Resolution\"]),\n",
    "            \"date_of_content\": clean_nulls(metadata[\"Date of Content\"]),\n",
    "            \"frequency_of_updates\": clean_nulls(metadata[\"Frequency of Updates\"]),\n",
    "            \n",
    "            \"learn_more_link\": clean_nulls(metadata[\"Learn More Link\"]),\n",
    "            \"data_download_link\": clean_nulls(metadata[\"Download Data (S3)\"]),\n",
    "            \"data_download_original_link\":clean_nulls(metadata[\"Download from Source\"])\n",
    "            \n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'content-type': \"application/json\",\n",
    "        'authorization': \"Bearer \" + auth_token,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        processed1.append(rw_id)\n",
    "        res = req.request(\"POST\", url, data=json.dumps(row_payload), headers = headers)\n",
    "        print(res)\n",
    "        #print(res.text)\n",
    "        if(\"already exists\" in res.text):\n",
    "            res = req.request(\"PATCH\", url, data=json.dumps(row_payload), headers = headers)\n",
    "            print(res)\n",
    "            if(\"errors:\" in res.text):\n",
    "                print(res.text)\n",
    "        elif(\"errors:\" in res.text):\n",
    "            print(res.text)\n",
    "    except TypeError as e:\n",
    "        print(e.args)\n",
    "        print(metadata[[\"Unique ID\", \"Public Title\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Merge subtitles with Tracking sheet\n",
    "\n",
    "#### UPLOADS TITLE, SUBTITLE, AND DOWNLOAD LINKS, if there is no METADATA IN METADATA FOR UPLOAD AVAILABEL ####\n",
    "\n",
    "# Keep only those datasets from trakcing sheet with rw_ids already\n",
    "tracking_valid_old_ids = pd.notnull(july_data_upload[old_id_col])\n",
    "tracking_valid_new_ids = pd.notnull(july_data_upload[new_id_col])\n",
    "\n",
    "july_data_upload[\"final_ids\"] = choose_new_id(july_data_upload, tracking_valid_old_ids,old_id_col, tracking_valid_new_ids,new_id_col)\n",
    "\n",
    "missed_ids = [rw_id for rw_id in july_data_upload[\"final_ids\"].values if ((rw_id not in current_mdata.index) and (rw_id != None))]\n",
    "\n",
    "missed_data = july_data_upload.reset_index().set_index(\"final_ids\")\n",
    "missed_data = missed_data.loc[missed_ids]\n",
    "missed_data\n",
    "\n",
    "\n",
    "### THIS ADDS ALL DATASETS FOR WHICH WE HAVE ENTRIES IN TRACKING SHEET and NOTHING IN METADATA FOR UPLOAD###\n",
    "print(\"True if below print empty list []\")\n",
    "print([ind for ind in missed_data.index if ind in current_mdata.index])\n",
    "\n",
    "processed2 = []\n",
    "\n",
    "for rw_id in missed_data.index:\n",
    "    url = \"https://api.resourcewatch.org/v1/dataset/\"+str(rw_id)+\"/metadata\"\n",
    "    # Everything from current_mdata\n",
    "    metadata = missed_data.loc[rw_id]\n",
    "    print(metadata[\"WRI Unique ID\"])\n",
    "    print(metadata[\"Public Title\"])\n",
    "    print(url)\n",
    "    #print(metadata)\n",
    "    row_payload = {\n",
    "        \"language\": \"en\",\n",
    "        \n",
    "        \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "        \"source\": clean_nulls(metadata[\"Subtitle\"]),\n",
    "        \n",
    "        \"application\":\"rw\",\n",
    "        \"dataset\":rw_id,\n",
    "        \n",
    "        \"info\": {\n",
    "            \n",
    "            \"wri_rw_id\": clean_nulls(metadata[\"WRI Unique ID\"]),\n",
    "\n",
    "            \"name\": clean_nulls(metadata[\"Public Title\"]),\n",
    "            \"technical_title\":clean_nulls(metadata[\"Technical Title\"]),\n",
    "\n",
    "            \"data_download_link\": clean_nulls(metadata[\"Download Data (S3)\"]), \n",
    "            \"data_download_original_link\": clean_nulls(metadata[\"Download from Source\"])\n",
    "            \n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'content-type': \"application/json\",\n",
    "        'authorization': \"Bearer \" + auth_token,\n",
    "    }\n",
    "    #print(row_payload)\n",
    "\n",
    "    try:\n",
    "        processed2.append(rw_id)\n",
    "        res = req.request(\"POST\", url, data=json.dumps(row_payload), headers = headers)\n",
    "        if(\"already exists\" in res.text):\n",
    "            res = req.request(\"PATCH\", url, data=json.dumps(row_payload), headers = headers)\n",
    "            if(\"errors:\" in res.text):\n",
    "                print(res.text)\n",
    "        elif(\"errors:\" in res.text):\n",
    "            print(res.text)\n",
    "    except TypeError as e:\n",
    "        print(e.args)\n",
    "        print(metadata[[\"Unique ID\", \"Public Title\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missed_data.to_csv(\"/Users/nathansuberi/Desktop/datasets_on_july_sheet_with_rw_id_no_metadata.csv\")\n",
    "missed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"9ea634db-53af-445e-a767-60ec9efc321e\" in processed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Inspect metadata on backoffice\n",
    "\n",
    "# Base URL for getting dataset metadata from RW API\n",
    "# Metadata = Data that describes Data \n",
    "url = \"https://api.resourcewatch.org/v1/dataset?sort=slug,-provider,userId&status=saved&includes=metadata,vocabulary,widget,layer\"\n",
    "\n",
    "# page[size] tells the API the maximum number of results to send back\n",
    "# There are currently between 200 and 300 datasets on the RW API\n",
    "payload = { \"application\":\"rw\", \"page[size]\": 1000}\n",
    "\n",
    "# Request all datasets, and extract the data from the response\n",
    "res = req.get(url, params=payload)\n",
    "data = res.json()[\"data\"]\n",
    "\n",
    "#############################################################\n",
    "\n",
    "### Convert the json object returned by the API into a pandas DataFrame\n",
    "# Another option: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "datasets_on_api = {}\n",
    "for ix, dset in enumerate(data):\n",
    "    atts = dset[\"attributes\"]\n",
    "    metadata = atts[\"metadata\"]\n",
    "    layers = atts[\"layer\"]\n",
    "    widgets = atts[\"widget\"]\n",
    "    tags = atts[\"vocabulary\"]\n",
    "    datasets_on_api[atts[\"name\"]] = {\n",
    "        \"rw_id\":dset[\"id\"],\n",
    "        \"upload_name\":atts[\"name\"],\n",
    "        \"table_name\":atts[\"tableName\"],\n",
    "        \"provider\":atts[\"provider\"],\n",
    "        \"date_updated\":atts[\"updatedAt\"],\n",
    "        \"num_metadata_keys\":len(metadata),\n",
    "        \"metadata\": metadata,\n",
    "        \"num_layers\":len(layers),\n",
    "        \"layers\": layers,\n",
    "        \"num_widgets\":len(widgets),\n",
    "        \"widgets\": widgets,\n",
    "        \"num_tags\":len(tags),\n",
    "        \"tags\":tags\n",
    "    }    \n",
    "    \n",
    "# Create the DataFrame, name the index, and sort by date_updated\n",
    "# More recently updated datasets at the top\n",
    "current_datasets_on_api = pd.DataFrame.from_dict(datasets_on_api, orient='index')\n",
    "\n",
    "def check_public_title(metadata):\n",
    "    if len(metadata) > 0:\n",
    "        mdata = metadata[0]\n",
    "        if \"attributes\" in mdata:\n",
    "            if \"info\" in mdata[\"attributes\"]:\n",
    "                if \"name\" in mdata[\"attributes\"][\"info\"]:\n",
    "                    return(mdata[\"attributes\"][\"info\"][\"name\"])\n",
    "        return(None)\n",
    "\n",
    "# Grab public title, if it exists in metadata\n",
    "current_datasets_on_api[\"public_title\"] = current_datasets_on_api.apply(lambda row: check_public_title(row[\"metadata\"]), axis=1)\n",
    "\n",
    "current_datasets_on_api.set_index(\"rw_id\", inplace=True)\n",
    "current_datasets_on_api.index.rename(\"Dataset\", inplace=True)\n",
    "current_datasets_on_api.sort_values(by=[\"date_updated\"], inplace=True, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### THIS COVERS ALL DATASETS WHICH ARE ON THE BACKOFFICE but HAVE NO WRI_ID / RW_ID IN TRACKING SHEET ###\n",
    "### Occasionally this is because the data has been moved to after launch\n",
    "\n",
    "investigate_mdata = current_datasets_on_api[[\"upload_name\", \"public_title\", \"metadata\"]]\n",
    "\n",
    "missed_ids = [rw_id for rw_id in investigate_mdata.index if ((rw_id not in processed1) & (rw_id not in processed2))]\n",
    "\n",
    "investigate_mdata = investigate_mdata.loc[missed_ids]\n",
    "\n",
    "investigate_mdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these are datasets for which the Unique ID changed\n",
    "\n",
    "soc.003 Distribution of Infant Mortality\n",
    "soc.016 Conflict and Protest Events in African...\n",
    "dis_007 Landslide Susceptibility Map\n",
    "bio.035 Coral Bleaching Frequency Prediction\n",
    "dis.001 Earthquakes Over the Past 30 days\n",
    "Foo_046a Food Footprint in Protein\n",
    "wat.033 Agriculture Water Demand and Depletion\n",
    "soc.062 Internal Displacement\n",
    "soc.061 Rural Poverty\n",
    "soc.042 Percentage of Urban Population with Ac\n",
    "soc.020 GINI Index\n",
    "soc.008 Gross Domestic Product Per Capita (PPP\n",
    "soc.006 Multidimensional Poverty Index\n",
    "soc.004 Human Development Index\n",
    "soc.002 Gender Development Index\n",
    "foo.002 GLDAS Land Water Content from NOAH Lan..\n",
    "com.028 Effect of Agricultural Policies on Com...\n",
    "cit.029 Municipal Waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DANGER Bug - able to update metadata for a dataset that no longer exists on the API\n",
    "#test upload cit.029:\n",
    "#    broken, old id: 8f14a33e-5a61-47af-b26e-c1fc036932a5\n",
    "#    working, new id: 00abb46f-34e2-4bf7-be30-1fb0b1de022f\n",
    "    \n",
    "url1=\"https://api.resourcewatch.org/v1/dataset/8f14a33e-5a61-47af-b26e-c1fc036932a5/metadata\"    \n",
    "url2=\"https://api.resourcewatch.org/v1/dataset/10337db6-8321-445e-a60b-28fc1e114f29/metadata\"\n",
    "\n",
    "res1a = req.request(\"POST\", url1, data=json.dumps(row_payload), headers = headers)\n",
    "if(\"already exists\" in res1a.text):\n",
    "    res1b = req.request(\"PATCH\", url1, data=json.dumps(row_payload), headers = headers)\n",
    "        \n",
    "res2a = req.request(\"POST\", url2, data=json.dumps(row_payload), headers = headers)\n",
    "if(\"already exists\" in res2a.text):\n",
    "    res2b = req.request(\"PATCH\", url1, data=json.dumps(row_payload), headers = headers)\n",
    "    \n",
    "print(res1b.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
