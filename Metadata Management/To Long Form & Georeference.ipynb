{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cartoframes\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "import requests as req\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from dateutil import parser\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets_in_wide_form = {} #{wri_id1:rw_id1, wri_id2, rw_id2, ...}\n",
    "\n",
    "###\n",
    "## Read from docker data volume - stored in json - can be hooked into API as microservice\n",
    "###\n",
    "\n",
    "# Long form table possibilities\n",
    "#datasets = {\"cit.022\":\"6b670396-c52c-430c-b5bb-20693da03b60\",\"cit.025\":\"d38d0d5c-31b1-47f4-9d2e-d8fba4c7d083\",\"cit.028\":\"35ce2b98-adbb-4873-b334-d7b1cc542de7\",\"cit.029\":\"10337db6-8321-445e-a60b-28fc1e114f29\",\"cli.007\":\"3d2ce960-abda-4c9c-bd29-1929e9ca24c9\",\"cli.008\":\"3a46f6b4-0eec-49d4-bbfc-e2e8f64e6117\",\"com.006\":\"2e31a1f3-576b-46b4-84f0-3f0cc399f887\",\"com.007\":\"fe311144-8c0e-4440-b068-6efd057e0f6a\",\"com.009\":\"c61c364b-1d68-4dd9-ae3d-76c2a0022280\",\"com.010\":\"52c55378-0484-48c3-92fc-3ee94d21c716\",\"com.015\":\"c18a38cd-94ff-48cd-818f-6ffb05992abb\",\"com.019\":\"5e3a3a9f-7380-47c0-ad84-2c193861e106\",\"com.028\":\"62c988a7-1e4d-418e-87bf-a743e24209e8\",\"ene.012\":\"d446a52e-c4c1-4e74-ae30-3204620a0365\",\"ene.021\":\"2c092793-aa3a-4520-959c-ad48165dcae4\",\"ene.022\":\"d639909f-bcf3-4875-b8c3-35f030b68ed3\",\"ene.028\":\"c665f519-eef9-4f67-a8bf-7e3e6dc8bfcd\",\"foo.006\":\"2034a766-6e8a-416d-b8ab-9b7b3e3abb15\",\"foo.015\":\"4338471d-881a-475f-8bd9-60c4d48b8e12\",\"foo.019\":\"8bc79a36-d77e-4ee3-b9bc-c77146cfc503\",\"foo.040\":\"91ff1359-6680-49bc-8002-20256e999993\",\"foo.041\":\"ccfb322a-20aa-4132-b58b-0f76acec8f5a\",\"foo.042\":\"7a551dd8-b59c-4f59-9d50-c92cb61c5799\",\"foo.043\":\"95b013a3-389a-4367-83b7-c9d68c28c406\",\"for.021\":\"05b7c688-09ba-4f33-90ea-185a1039df43\",\"soc.004\":\"bea122ce-1e4b-465d-8b7b-fa11aadd20f7\",\"cit.017\":\"0303127a-70b0-4164-9251-d8162615d058\",\"soc.005\":\"a7067e9f-fe40-4338-85da-13a6071c76fe\",\"soc.006\":\"a89c95c7-0b82-4162-b9d8-cc0205e9f7ec\",\"soc.008\":\"00abb46f-34e2-4bf7-be30-1fb0b1de022f\",\"soc.015\":\"e8f53f73-d77c-485a-a2a6-1c47ea4aead9\",\"soc.020\":\"f8d3e79c-c3d0-4f9a-9b68-9c5ad1f025e4\",\"soc.023\":\"d3a6b89f-cf5c-40cf-b2b3-ac1c8315c648\",\"soc.029\":\"7793f46c-a48a-466f-a8ce-ca1a87b7aeed\",\"soc.035\":\"e7780d53-ad80-45bd-a271-79615ee97a37\",\"soc.036\":\"8671f536-1979-4b6f-a147-70152fcb44ed\",\"soc.039\":\"b37048be-9b23-4458-a047-888956c69aa1\",\"soc.040\":\"37d04efc-0ab2-4499-a891-54dca1013c74\",\"soc.062\":\"5e69cfac-1f68-4864-a19a-3c1bdb180100\",\"wat.005\":\"1b97e47e-ca18-4e50-9aae-a2853acca3f0\",\"cli.029\":\"fa6443ff-eb95-4d0f-84d2-f0c91682efdf\"}\n",
    "datasets = {\"foo.013\":\"45c3a4cb-2425-41c8-bb4b-11fcbb8bc998\"}\n",
    "datasets_in_wide_form = datasets\n",
    "datasets_to_geolocate_with_rw_countries = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cit.022', 'cit.025', 'cit.028', 'cit.029', 'cli.007', 'cli.008', 'com.006', 'com.007', 'com.009', 'com.010', 'com.015', 'com.019', 'com.028', 'ene.012', 'ene.021', 'ene.022', 'ene.028', 'foo.006', 'foo.015', 'foo.019', 'foo.040', 'foo.041', 'foo.042', 'foo.043', 'for.021', 'soc.004', 'cit.017', 'soc.005', 'soc.006', 'soc.008', 'soc.015', 'soc.020', 'soc.023', 'soc.029', 'soc.035', 'soc.036', 'soc.039', 'soc.040', 'soc.062', 'wat.005', 'cli.029'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"cit.022\":\"6b670396-c52c-430c-b5bb-20693da03b60\",\"cit.025\":\"d38d0d5c-31b1-47f4-9d2e-d8fba4c7d083\",\"cit.028\":\"35ce2b98-adbb-4873-b334-d7b1cc542de7\",\"cit.029\":\"10337db6-8321-445e-a60b-28fc1e114f29\",\"cli.007\":\"3d2ce960-abda-4c9c-bd29-1929e9ca24c9\",\"cli.008\":\"3a46f6b4-0eec-49d4-bbfc-e2e8f64e6117\",\"com.006\":\"2e31a1f3-576b-46b4-84f0-3f0cc399f887\",\"com.007\":\"fe311144-8c0e-4440-b068-6efd057e0f6a\",\"com.009\":\"c61c364b-1d68-4dd9-ae3d-76c2a0022280\",\"com.010\":\"52c55378-0484-48c3-92fc-3ee94d21c716\",\"com.015\":\"c18a38cd-94ff-48cd-818f-6ffb05992abb\",\"com.019\":\"5e3a3a9f-7380-47c0-ad84-2c193861e106\",\"com.028\":\"62c988a7-1e4d-418e-87bf-a743e24209e8\",\"ene.012\":\"d446a52e-c4c1-4e74-ae30-3204620a0365\",\"ene.021\":\"2c092793-aa3a-4520-959c-ad48165dcae4\",\"ene.022\":\"d639909f-bcf3-4875-b8c3-35f030b68ed3\",\"ene.028\":\"c665f519-eef9-4f67-a8bf-7e3e6dc8bfcd\",\"foo.006\":\"2034a766-6e8a-416d-b8ab-9b7b3e3abb15\",\"foo.015\":\"4338471d-881a-475f-8bd9-60c4d48b8e12\",\"foo.019\":\"8bc79a36-d77e-4ee3-b9bc-c77146cfc503\",\"foo.040\":\"91ff1359-6680-49bc-8002-20256e999993\",\"foo.041\":\"ccfb322a-20aa-4132-b58b-0f76acec8f5a\",\"foo.042\":\"7a551dd8-b59c-4f59-9d50-c92cb61c5799\",\"foo.043\":\"95b013a3-389a-4367-83b7-c9d68c28c406\",\"for.021\":\"05b7c688-09ba-4f33-90ea-185a1039df43\",\"soc.004\":\"bea122ce-1e4b-465d-8b7b-fa11aadd20f7\",\"cit.017\":\"0303127a-70b0-4164-9251-d8162615d058\",\"soc.005\":\"a7067e9f-fe40-4338-85da-13a6071c76fe\",\"soc.006\":\"a89c95c7-0b82-4162-b9d8-cc0205e9f7ec\",\"soc.008\":\"00abb46f-34e2-4bf7-be30-1fb0b1de022f\",\"soc.015\":\"e8f53f73-d77c-485a-a2a6-1c47ea4aead9\",\"soc.020\":\"f8d3e79c-c3d0-4f9a-9b68-9c5ad1f025e4\",\"soc.023\":\"d3a6b89f-cf5c-40cf-b2b3-ac1c8315c648\",\"soc.029\":\"7793f46c-a48a-466f-a8ce-ca1a87b7aeed\",\"soc.035\":\"e7780d53-ad80-45bd-a271-79615ee97a37\",\"soc.036\":\"8671f536-1979-4b6f-a147-70152fcb44ed\",\"soc.039\":\"b37048be-9b23-4458-a047-888956c69aa1\",\"soc.040\":\"37d04efc-0ab2-4499-a891-54dca1013c74\",\"soc.062\":\"5e69cfac-1f68-4864-a19a-3c1bdb180100\",\"wat.005\":\"1b97e47e-ca18-4e50-9aae-a2853acca3f0\",\"cli.029\":\"fa6443ff-eb95-4d0f-84d2-f0c91682efdf\"}.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-d623ccfe2f10>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-d623ccfe2f10>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    aws_access_key_id = #os.environ.get('aws_access_key_id')\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "###\n",
    "## Set up S3 access ###\n",
    "### \n",
    "\n",
    "\n",
    "\n",
    "aws_access_key_id = #os.environ.get('aws_access_key_id')\n",
    "aws_secret_access_key = #os.environ.get('aws_secret_access_key')\n",
    "\n",
    "s3_bucket = \"wri-public-data\"\n",
    "s3_folder = \"resourcewatch/wide_to_long/\"\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Functions for reading and uploading data to/from S3\n",
    "def read_from_S3(bucket, key, index_col=0):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(io.BytesIO(obj['Body'].read()), index_col=[index_col], encoding=\"utf8\")\n",
    "    return(df)\n",
    "\n",
    "def write_to_S3(df, bucket, key):\n",
    "    csv_buffer = io.StringIO()\n",
    "    # Need to set encoding in Python2... default of 'ascii' fails\n",
    "    df.to_csv(csv_buffer, encoding='utf-8')\n",
    "    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "###\n",
    "## Set up carto context ###\n",
    "###\n",
    "\n",
    "CARTO_USER = 'wri-rw'#os.environ.get('CARTO_USER')\n",
    "CARTO_KEY = ''#os.environ.get('CARTO_KEY')\n",
    "cc = cartoframes.CartoContext(base_url='https://{}.carto.com/'.format(CARTO_USER),\n",
    "                              api_key=CARTO_KEY)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "## Access data from API\n",
    "###\n",
    "\n",
    "\n",
    "# Base URL for getting dataset metadata from RW API\n",
    "url = \"https://api.resourcewatch.org/v1/dataset?sort=slug,-provider,userId&status=saved&includes=metadata,vocabulary,widget,layer\"\n",
    "\n",
    "# page[size] tells the API the maximum number of results to send back\n",
    "# There are currently between 200 and 300 datasets on the RW API\n",
    "payload = { \"application\":\"rw\", \"page[size]\": 1000}\n",
    "\n",
    "# Request all datasets, and extract the data from the response\n",
    "res = req.get(url, params=payload)\n",
    "data = res.json()[\"data\"]\n",
    "\n",
    "### Convert the json object returned by the API into a pandas DataFrame\n",
    "# Another option: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "datasets_on_api = {}\n",
    "for ix, dset in enumerate(data):\n",
    "    atts = dset[\"attributes\"]\n",
    "    metadata = atts[\"metadata\"]\n",
    "    layers = atts[\"layer\"]\n",
    "    widgets = atts[\"widget\"]\n",
    "    tags = atts[\"vocabulary\"]\n",
    "    datasets_on_api[dset[\"id\"]] = {\n",
    "        \"name\":atts[\"name\"],\n",
    "        \"table_name\":atts[\"tableName\"],\n",
    "        \"provider\":atts[\"provider\"],\n",
    "        \"date_updated\":atts[\"updatedAt\"],\n",
    "        \"num_metadata\":len(metadata),\n",
    "        \"metadata\": metadata,\n",
    "        \"num_layers\":len(layers),\n",
    "        \"layers\": layers,\n",
    "        \"num_widgets\":len(widgets),\n",
    "        \"widgets\": widgets,\n",
    "        \"num_tags\":len(tags),\n",
    "        \"tags\":tags\n",
    "    }\n",
    "\n",
    "# Create the DataFrame, name the index, and sort by date_updated\n",
    "# More recently updated datasets at the top\n",
    "current_datasets_on_api = pd.DataFrame.from_dict(datasets_on_api, orient='index')\n",
    "current_datasets_on_api.index.rename(\"Dataset\", inplace=True)\n",
    "current_datasets_on_api.sort_values(by=[\"date_updated\"], inplace=True, ascending = False)\n",
    "\n",
    "# Select all Carto datasets on the API:\n",
    "provider = \"cartodb\"\n",
    "carto_ids = (current_datasets_on_api[\"provider\"]==provider)\n",
    "carto_data = current_datasets_on_api.loc[carto_ids]\n",
    "\n",
    "logging.info(\"Number of Carto datasets: \" + str(carto_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load config options\n",
    "try:\n",
    "    table_info = json.load(open('./table_info.json', 'r'))\n",
    "except Exception as e:\n",
    "    logging.info(e)\n",
    "    table_info = {}\n",
    "table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### This wrapper allows for only overwriting the 'data' table below... before had to wipe long_data to rerun this\n",
    "\n",
    "def ddwrapper():\n",
    "    return defaultdict(lambda : pd.DataFrame())\n",
    "\n",
    "tables = defaultdict(ddwrapper)\n",
    "\n",
    "tables['geometry'] = {\n",
    "    'data':cc.read('wri_countries_a')\n",
    "}\n",
    "table_info['geometry'] = {\n",
    "    'config_options':{}\n",
    "}\n",
    "\n",
    "tables['country_aliases'] = {\n",
    "    'old_data':cc.read('rw_aliasing_countries'),\n",
    "    'data':cc.read('country_aliases_extended')\n",
    "}\n",
    "table_info['country_aliases'] = {\n",
    "    'config_options':{}\n",
    "}\n",
    "\n",
    "\n",
    "###\n",
    "## This is currently built out to read data already on the API... \n",
    "## could be used to prepare data for upload to the API\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "for rw_id, api_id in datasets_in_wide_form.items():\n",
    "    logging.info(rw_id)\n",
    "\n",
    "    # Fetch table name\n",
    "    try:\n",
    "        table_name = carto_data.loc[api_id]['table_name'].replace('%20', '')\n",
    "    except:\n",
    "        msg = '***** ' + rw_id + ' not in carto_data *****'\n",
    "        table_info[table_name] = {\n",
    "            'error':msg\n",
    "        }\n",
    "        logging.info(msg)\n",
    "\n",
    "    '''\n",
    "    if table_name in table_info:\n",
    "        if 'config_options' in table_info[table_name]:\n",
    "            if 'prefixes' in table_info[table_name]['config_options']:\n",
    "                if table_info[table_name]['config_options']['prefixes'] in [[u'--'], [u'already long']]:\n",
    "                    logging.info('not fetching data for ' + table_name)\n",
    "                    continue\n",
    "    '''\n",
    "\n",
    "    # Fetch data\n",
    "    try:\n",
    "        tables[table_name]['data'] = cc.read(table_name)\n",
    "        if 'config_options' not in table_info[table_name]:\n",
    "            logging.info('updating table_info for ' + table_name)\n",
    "            table_info[table_name] = {\n",
    "                'config_options':{}\n",
    "            }\n",
    "    except:\n",
    "        msg = '***** Problem fetching ' + table_name + ' from Carto server *****'\n",
    "        if table_name in table_info:\n",
    "            if 'error' in table_info[table_name]:\n",
    "                logging.info(table_info[table_name]['error'])\n",
    "        else:\n",
    "            table_info[table_name] = {\n",
    "                'error':msg\n",
    "            }\n",
    "            logging.info(msg)\n",
    "\n",
    "\n",
    "for name, info in tables.items():\n",
    "    logging.info('table name: ' + name)\n",
    "    try:\n",
    "        logging.info('table shape: ' + str(info['data'].shape))\n",
    "    except:\n",
    "        logging.info(info['error'])\n",
    "\n",
    "table_info['soc_023_fragile_states_index'] = {\n",
    "    'config_options':{'prefixes':['rank', 'score'], 'country_code':False, 'country_name':'country',\n",
    "                     'id_cols':['country']}\n",
    "}\n",
    "\n",
    "table_info['ene_022_se4all_tracking_goals']['config_options']['id_cols'] = ['series_name', 'series_code', \n",
    "                                                                            'country_name', 'country_code']\n",
    "\n",
    "table_info['foo_006_effects_climate_change_food_production']['config_options']['id_cols'] = ['bls_2_countries__sres__abbrevname', \n",
    "                                                                                            'fips_code', 'iso3v10',\n",
    "                                                                                            'wh_2000', 'ri_2000', 'mz_2000',\n",
    "                                                                                            'country','wh_gr', 'ri_gr', 'mz_gr']\n",
    "table_info['foo_006_effects_climate_change_food_production']['config_options']['country_code'] = 'iso3v10'\n",
    "table_info['foo_006_effects_climate_change_food_production']['config_options']['country_name'] = 'country'\n",
    "\n",
    "table_info['soc_023_fragile_states_index']['config_options']['_incl'] = True\n",
    "table_info['soc_023_fragile_states_index']['config_options']['country_code'] = False\n",
    "table_info['soc_023_fragile_states_index']['config_options']['country_name'] = 'country'\n",
    "table_info['soc_023_fragile_states_index']['config_options']['id_cols'] = ['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_info['foo_013consumptiontest'] = {\n",
    "    'config_options':{\n",
    "        'country_code':False,\n",
    "        'country_name':'country',\n",
    "        'id_cols':['country'],\n",
    "        'prefixes':['animal_prod_energy_cons_', 'meat_energy_cons_',\n",
    "           'ruminant_meat_energy_cons_', 'sum_energy_cons_'],\n",
    "        '_incl':False,\n",
    "        'parse_date':True\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_cols = ['animal_prod_energy_cons_2008', 'meat_energy_cons_2008',\n",
    "           'ruminant_meat_energy_cons_2008', 'sum_energy_cons_2008',\n",
    "            'sum_energy_cons_2050','animal_prod_energy_cons_2050',\n",
    "           'meat_energy_cons_2050','ruminant_meat_energy_cons_2050',\n",
    "           'cartodb_georef_status','country', 'the_geom']\n",
    "tables['foo_013consumptiontest']['data'].columns = new_cols\n",
    "table_info['foo_013consumptiontest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables['foo_013consumptiontest']['data'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add config options\n",
    "for name, info in tables.items():\n",
    "    logging.info(name)\n",
    "    logging.info(list(info['data'].columns))\n",
    "    print\n",
    "\n",
    "    ## To determine id columns to use, uncomment\n",
    "    '''\n",
    "    if not table_info[name]['config_options'].get('id_cols', False):   \n",
    "    id_cols = input('id columns? separate by comma with a space')\n",
    "    table_info[name]['config_options']['id_cols'] = [col.strip() for col in id_cols.split(',')]\n",
    "    '''\n",
    "    \n",
    "    ## To assign prefixes for value column names, uncomment\n",
    "    '''\n",
    "    prefixes = input('time column prefixes?')\n",
    "    table_info[name]['config_options']['prefixes'] = [pfx.strip() for pfx in prefixes.split(',')]\n",
    "    '''\n",
    "\n",
    "    ## To determine whether the value column names have a _ after the prefix, uncomment\n",
    "    '''\n",
    "    _included = input('Is there a _ after the prefix? type anything for True, leave blank for False')\n",
    "    if _included != '':\n",
    "        table_info[name]['config_options']['_incl'] = True\n",
    "    else:\n",
    "        table_info[name]['config_options']['_incl'] = False\n",
    "    '''\n",
    "\n",
    "    ## To determine country identifying info column, uncomment\n",
    "    '''\n",
    "    if table_info[name]['config_options'].get('prefixes', False) == ['already long']:\n",
    "        country_code = input('country code column? leave blank if needs country code added')\n",
    "        if country_code != '':\n",
    "            table_info[name]['config_options']['country_code'] = country_code\n",
    "        else:\n",
    "            table_info[name]['config_options']['country_code'] = False\n",
    "\n",
    "        country_name = input('country name column? leave blank if needs country name added')\n",
    "        if country_name != '':\n",
    "            table_info[name]['config_options']['country_name'] = country_name\n",
    "        else:\n",
    "            table_info[name]['config_options']['country_name'] = False\n",
    "    '''\n",
    "    \n",
    "    ## Auto parse datetime\n",
    "    '''\n",
    "    parse_datetime = input('auto parse datetime? type anything for True, leave blank for False')\n",
    "    if parse_datetime != '':\n",
    "        table_info[name]['config_options']['parse_date'] = True\n",
    "    else:\n",
    "        table_info[name]['config_options']['parse_date'] = False\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what we're working with, we can get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save this info for working with later\n",
    "json.dump(table_info, open('./table_info.json', 'w'))\n",
    "table_info = json.load(open('./table_info.json', 'r'))\n",
    "table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use known prefixes to reformat tables\n",
    "def pick_value_col(col, pfx, use_):\n",
    "    if use_:\n",
    "        if pfx+'_' in col:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if pfx in col:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def pick_value_wrapper(col, pfx, use_, xtra):\n",
    "    return pick_value_col(col,pfx,use_) & (len(col)==xtra+len(pfx))\n",
    "    \n",
    "def prepare_date(date, pfx, use_, parse_date):\n",
    "    if use_:\n",
    "        if parse_date:\n",
    "            dt = parser.parse(date[date.index(pfx) + len(pfx) + 1:])\n",
    "            dt = dt.replace(month=1)\n",
    "            return dt.replace(day=1)\n",
    "        else:\n",
    "            return date[date.index(pfx) + len(pfx) + 1:]\n",
    "    else:\n",
    "        if parse_date:\n",
    "            dt = parser.parse(date[date.index(pfx) + len(pfx):])\n",
    "            dt = dt.replace(month=1)\n",
    "            return dt.replace(day=1)\n",
    "        else:\n",
    "            return date[date.index(pfx) + len(pfx):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, info in tables.items():\n",
    "    \n",
    "    if not table_info[name]['config_options'].get('prefixes', False):\n",
    "        continue\n",
    "    if table_info[name]['config_options']['prefixes'] == ['already long']:\n",
    "        logging.info('transfering ' + name + ', already in long form')\n",
    "        tables[name]['long_data'] = info['data']\n",
    "        continue\n",
    "    if table_info[name]['config_options']['prefixes'] == ['--']:\n",
    "        logging.info('skipping ' + name + ', requires more manual approach')\n",
    "        continue\n",
    "\n",
    "    logging.info('processing ' + name)\n",
    "    logging.info('options:')\n",
    "    logging.info(table_info[name]['config_options'])\n",
    "    \n",
    "    prefixes = table_info[name]['config_options'].get('prefixes', [])\n",
    "    id_cols = table_info[name]['config_options'].get('id_cols', [])\n",
    "    use_ = table_info[name]['config_options'].get('_incl', False)\n",
    "    parse_date = table_info[name]['config_options'].get('parse_date', False)\n",
    "\n",
    "    logging.info('initial shape: ' + str(info['data'].shape))\n",
    "    \n",
    "    df = pd.DataFrame(columns = id_cols + ['variable'])\n",
    "    \n",
    "    for pfx in prefixes:\n",
    "        logging.debug('working on pfx ' + pfx)\n",
    "        \n",
    "        if any([ds in name for ds in ['soc_015']]):\n",
    "            value_cols = [col for col in info['data'].columns if pick_value_wrapper(col, pfx, use_, 5)]\n",
    "        elif any([ds in name for ds in ['foo_006']]):\n",
    "            value_cols = [col for col in info['data'].columns if pick_value_wrapper(col, pfx, use_, 4)]\n",
    "        else:\n",
    "            value_cols = [col for col in info['data'].columns if pick_value_col(col, pfx, use_)]\n",
    "        logging.debug('columns pulled: ' + str(value_cols))\n",
    "\n",
    "        _df = pd.melt(info['data'], id_vars=id_cols, value_vars=value_cols)\n",
    "        _df['variable'] = [prepare_date(date, pfx, use_, parse_date) for date in _df['variable']]\n",
    "\n",
    "        col_names = [pfx+'_data' if col=='value' else col for col in _df.columns]\n",
    "        _df.columns = col_names\n",
    "\n",
    "        df = df.merge(_df, on=id_cols  + ['variable'], how='outer')\n",
    "        logging.debug('intermediate df shape: ' + str(df.shape))\n",
    "\n",
    "    logging.info('final shape of ' + name + ': ' + str(df.shape))\n",
    "\n",
    "    new_cols = ['datetime' if col=='variable' else col for col in df.columns]\n",
    "    df.columns = new_cols\n",
    "    logging.info('final columns: ' + str(df.columns))\n",
    "    \n",
    "    tables[name]['long_data'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUCCESS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets_to_geolocate_with_rw_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore geometries and aliasing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables['geometry']['data'].head(1)\n",
    "tables['geometry']['data'][tables['geometry']['data']['adm0_a3']=='MLI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables['country_aliases']['data'].head(1)\n",
    "tables['country_aliases']['data'][tables['country_aliases']['data']['alias']=='france']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep work to get country alias table ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "## Configuring the alias table\n",
    "###\n",
    "\n",
    "df = tables['country_aliases']['data']\n",
    "logging.info(df.columns)\n",
    "try:\n",
    "    df = df.drop('the_geom', axis=1)\n",
    "except:\n",
    "    logging.info('could not delete column `the_geom`')\n",
    "\n",
    "\n",
    "try:\n",
    "    df = df.drop('index', axis=1)\n",
    "except:\n",
    "    logging.info('could not delete column `index`')\n",
    "\n",
    "\n",
    "logging.info(df.columns)\n",
    "\n",
    "## Adding all countries from our wri-bounds shapefile to the alias table\n",
    "\n",
    "new_aliases = tables['geometry']['data'][['iso_a3', 'name']].copy()\n",
    "new_aliases['alias'] = new_aliases['name']\n",
    "cols = ['iso' if col=='iso_a3' else col for col in new_aliases.columns]\n",
    "cols = [col.strip() for col in cols]\n",
    "new_aliases.columns = cols\n",
    "\n",
    "logging.info(df.columns)\n",
    "logging.info(df.shape)\n",
    "logging.info(new_aliases.columns)\n",
    "logging.info(new_aliases.shape)\n",
    "\n",
    "df = df.append(new_aliases)\n",
    "\n",
    "## Adding in new aliases identified by Peter\n",
    "\n",
    "peters_new_aliases = pd.read_csv('aliases_for_longform.csv', header=1)\n",
    "peters_new_aliases.columns = ['alias', 'name', 'iso']\n",
    "\n",
    "df = df.append(peters_new_aliases)\n",
    "\n",
    "# Make all aliases lower case, remove spacing\n",
    "df['alias'] = [alias.strip().lower().replace(' ','') for alias in df['alias']]\n",
    "\n",
    "## check / remove duplicates\n",
    "sum(df.duplicated(subset=['alias']))\n",
    "sum(df.duplicated(subset=['name']))\n",
    "sum(df.duplicated(subset=['iso']))\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "    \n",
    "tables['country_aliases']['data'] = df\n",
    "\n",
    "logging.info('Size of current aliasing table: ' + str(tables['country_aliases']['data'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[df['name'].str.lower().str.contains('moldova')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tracking all mis-matched names\n",
    "missed_names = []\n",
    "missed_isos = []\n",
    "\n",
    "alias_info = tables['country_aliases']['data']\n",
    "\n",
    "for name, info in tables.items():\n",
    "\n",
    "    if name in ['geometry', 'country_aliases']:\n",
    "        continue\n",
    "    if name != '':\n",
    "        #continue\n",
    "        pass\n",
    "    \n",
    "    logging.info('Processing table ' + name)\n",
    "    logging.info('Table head:')\n",
    "    \n",
    "    ### WARNING: non standardized indices in the data cause problems after the merge step\n",
    "    if 'long_data' in info:\n",
    "        data = info['long_data'].copy()\n",
    "        data.index = list(range(data.shape[0]))\n",
    "    else:\n",
    "        data = info['data'].copy()\n",
    "        data.index = list(range(data.shape[0]))\n",
    "    \n",
    "    logging.info(data.head(5))\n",
    "\n",
    "    c_code = table_info[name]['config_options'].get('country_code', False)\n",
    "    c_name = table_info[name]['config_options'].get('country_name', False)\n",
    "    \n",
    "    logging.info('c_code: ***' + str(c_code) + '***')\n",
    "    logging.info('c_name: ***' + str(c_name) + '***')\n",
    "    \n",
    "    \n",
    "    # Check if isos match our table\n",
    "    \n",
    "    if c_code:\n",
    "        logging.info('already has an iso3 code, in column ' + c_code)\n",
    "        \n",
    "        df = data.merge(alias_info,\n",
    "                           left_on=c_code,\n",
    "                           right_on='iso', \n",
    "                           how='left')\n",
    "        \n",
    "        null_isos = pd.isnull(df['iso'])\n",
    "        \n",
    "        no_iso_match = df[null_isos.values]\n",
    "        logging.info('no match for these isos in the data being processed: ')\n",
    "        logging.info(no_iso_match[c_code].unique())\n",
    "        missed_isos.extend(no_iso_match[c_code].unique())\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # If country name is supplied, check how many match up with alias/name in country_aliases\n",
    "    if c_name:       \n",
    "        # Ensure that leading or trailing spaces don't break the match\n",
    "        data[c_name] = ['North Korea' if name=='Korea, Dem. People\\x92s Rep.' else name for name in data[c_name]]\n",
    "    \n",
    "        _data = data.copy()\n",
    "        _data['join_col'] = data[c_name].apply(lambda item: item.strip().lower().replace(' ',''))\n",
    "    \n",
    "        data_with_alias = _data.merge(alias_info, \n",
    "                                         left_on = 'join_col',\n",
    "                                         right_on = 'alias',\n",
    "                                         how='left') \n",
    "\n",
    "        null_aliases = pd.isnull(data_with_alias['alias'])             \n",
    "            \n",
    "        logging.info('data with alias df:')\n",
    "        logging.info(data_with_alias.shape)\n",
    "        logging.info(data_with_alias.head(6))\n",
    "        logging.info('raw data')\n",
    "        logging.info(data.shape)\n",
    "        logging.info(data.head(5))\n",
    "    \n",
    "        \n",
    "        if sum(null_aliases):\n",
    "            no_alias_match = data_with_alias.loc[null_aliases]\n",
    "            logging.info('missed aliases, matching on column \"alias\" of country_aliases')\n",
    "            logging.info(no_alias_match)\n",
    "            try:\n",
    "                logging.info(no_alias_match[c_name].unique())\n",
    "                missed_names.extend(no_alias_match[c_name].unique())\n",
    "            except:\n",
    "                c_name = c_name+'_x'\n",
    "                logging.info(no_alias_match[c_name].unique())\n",
    "                missed_names.extend(no_alias_match[c_name].unique())\n",
    "                \n",
    "        ### data IS ALTERED HERE\n",
    "\n",
    "        try:\n",
    "            data['rw_country_code'] = data_with_alias['iso']\n",
    "        except:\n",
    "            data['rw_country_code'] = data_with_alias['iso_y']\n",
    "            \n",
    "        try:\n",
    "            data['rw_country_name'] = data_with_alias['name']  \n",
    "        except:\n",
    "            data['rw_country_name'] = data_with_alias['name_y'] \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    ### SUCCESS\n",
    "    logging.info('Final head:')\n",
    "    logging.info(data.head(5))\n",
    "    tables[name]['geo_data'] = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = tables['cit_028_percent_urban_slums_edit']['data']\n",
    "df[df['country_code']=='CHI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missed_names = list(set(missed_names))\n",
    "print(len(missed_names))\n",
    "print(missed_names)\n",
    "\n",
    "\n",
    "missed_isos = list(set(missed_isos))\n",
    "print(len(missed_isos))\n",
    "print(missed_isos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in missed_names:\n",
    "    for alias in tables['country_aliases']['data']['alias']:\n",
    "        if alias.lower().replace(' ', '') in name.lower().replace(' ', ''):\n",
    "            print(name)\n",
    "df = tables['country_aliases']['data']  \n",
    "df.loc[df['name'].str.lower().str.contains('cur')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iso in missed_isos:\n",
    "    for iso in tables['geometry']['data']['adm0_a3']:\n",
    "        if alias.lower().replace(' ', '') in name.lower().replace(' ', ''):\n",
    "            print(name)\n",
    "            \n",
    "for iso in missed_isos:\n",
    "    for iso in tables['country_aliases']['data']['iso']:\n",
    "        if alias.lower().replace(' ', '') in name.lower().replace(' ', ''):\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_to_S3(tables['country_aliases']['data'],s3_bucket,s3_folder+'country_aliases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc.write(tables['country_aliases']['data'], 'country_aliases_extended', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, info in tables.items():\n",
    "    if name in ['geometry', 'country_aliases']:\n",
    "        continue\n",
    "        #pass\n",
    "    if name != '':\n",
    "        #continue\n",
    "        pass\n",
    "    \n",
    "    logging.info(name)\n",
    "    for key, data in info.items():\n",
    "        if key == 'geo_data':\n",
    "            logging.info(data.head())\n",
    "            write_to_S3(data,s3_bucket,s3_folder+name+'_long')\n",
    "            logging.info('saved ' + name + ' long data to s3')\n",
    "            cc.write(data, name + '_georefed_&_longform', overwrite=True)\n",
    "            logging.info('saved ' + name + ' long data to Carto')\n",
    "            #logging.info('failed to write table ' + name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
